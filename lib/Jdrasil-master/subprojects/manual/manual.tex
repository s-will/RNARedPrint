\documentclass[a4paper, ukenglish, twoside, openright]{jdrasilmanual}

\setversion{0.1}
\author{Max Bannach, Sebastian Berndt, Thorsten Ehlers}

\begin{document}

\part{Getting Started}
%\chapter{About \Jdrasil}
\section{Technical Overview and Design Principles}
\Jdrasil{} is a modular Java library for computing tree
decompositions both, exact and heuristically. The goal of \Jdrasil\ is
to allow other projects to add tree decompositions to their
applications as easy as possible. In order to achieve this, \Jdrasil\
is designed in a very modular way: Every algorithm is implemented as
interchangeable as possible. At the same time, algorithms are
implemented in a clean object oriented manner, making it easy to
understand and extend the implementation. We hope that this approach
makes it easier to study the practical aspects of tree width
algorithms, and that it helps to push the practical usage of tree
decompositions.

To make the usage of \Jdrasil\ as easy as possible, the whole library can be
compiled and used without any dependencies. Everything boils down to a
single, platform independent \texttt{jar} file that can freely be used
with the mentioned copyright.

However, this design principle comes with the price of algorithms that
are not ``on the edge optimized''. We believe that this is not a
problem for most applications, as the algorithms run in the same
principle time complexity anyway. If we, however, wish to use super
optimized algorithms, \Jdrasil{} provides the concept of
\emph{upgrades}. An upgrade is a piece of third party software
(e.\,g., another Java library, an optimized C implementation of an
algorithm, a SAT solver, \dots) that can be used to \emph{replace} some
functionality of \Jdrasil. An upgrade will usually boost the
performance of \Jdrasil\ in general, or on a specific target platform
(for instance on a parallel machine), and will add dependencies
(\Jdrasil\ with upgrades may not be platform
independent). Nevertheless, an upgrade will \emph{not} increase the
functionality of \Jdrasil, only the performance and, hence, a scripted
based on an upgraded version of \Jdrasil\ is still platform
independent.

\section{Contributors}
The core developer of \Jdrasil\ are (in lexicographical order):
\begin{itemize}
  \item Max Bannach
  \item Sebastian Berndt
  \item Thorsten Ehlers
\end{itemize}
\section{How to Cite \Jdrasil}
If you use \Jdrasil\ for your project, please refer to our GitHub
page and respect the copyright. If you use \Jdrasil\ for a research
project, please cite our introduction paper:
\begin{lstlisting}[language=BibTeX]
  @inproceedings{jdrasil,
    author = {Max Bannach and Sebastian Berndt and Thorsten Ehlers},
    title = {Jdrasil: A Modular Library for Computing Tree Decompositions},
    booktitle = {tba},
    year = 2017
  }
\end{lstlisting}
\chapter{Build and Run \Jdrasil}
\section{Obtain and Use the Latest Version}
The latest version of \Jdrasil{} can be obtained from
\url{https://maxbannach.github.io/Jdrasil/}. The obtained
\texttt{.jar} file is executable, i.\,e., if \Jdrasil\ should be used
as standalone solver, we can simple use:
\begin{lstlisting}[language=bash]
  # this will execute the exact mode
  java -jar Jdrasil.jar
\end{lstlisting}
or alternatively
\begin{lstlisting}[language=bash]
  java -cp Jdrasil.jar jdrasil.Exact
\end{lstlisting}
In addition, we can execute \Jdrasil\ in the heuristic or
approximation mode:
\begin{lstlisting}[language=bash]
  java -cp Jdrasil.jar jdrasil.Heuristic
  java -cp Jdrasil.jar jdrasil.Approximation
\end{lstlisting}
Note that the heuristic mode tries to improve the solution
until a \texttt{SIGTERM} is received, i.\,e., \Jdrasil\ has to be stopped
manually in this mode. 

With the same \texttt{.jar} file, \Jdrasil\ can also be used as
library: simply add the \texttt{.jar} to the classpath of the desired
project. For instance, the following simple program uses \Jdrasil\ to
compute an exact tree decomposition:
\begin{lstlisting}[language=Java]
  import jdrasil.graph.*;
  import jdrasil.algorithms.*;

  public class Main {
    public static void main(String[] args) {
      // create your own graph
      Graph<Integer> G = GraphFactory.emptyGraph();
      for (int v = 1; v <= 5; v++) { G.addVertex(v); }
      G.addEdge(1, 2);
      G.addEdge(1, 4);
      G.addEdge(2, 3);
      G.addEdge(2, 4);
      G.addEdge(4, 5);

      // output graph in PACE format
      System.out.println(G);
      
      // compute exact decomposition
      TreeDecomposition<Integer> td = null;
      try {
        td = new ExactDecomposer<Integer>(G).call();
      } catch (Exception e) {
        System.out.println("something went wrong");
      }

      // ouput tree decomposition in PACE format
      System.out.println(td);
    }
  }
\end{lstlisting}
If the program is stored in a file \file{Main.java}, it can be
compiled and used with the following commands:
\begin{lstlisting}[language=bash]
  javac -cp Jdrasil.jar: Main.java
  java -cp Jdrasil.jar: Main
\end{lstlisting}
\section{Program Arguments}
The behavior of \Jdrasil\ can be influenced by program arguments. The
following tables provides an overview of all available arguments:
\begin{center}
  \def\check{\textcolor{jdrasil.fg}{\checkmark}}
  \begin{longtable}{lp{5cm}ccc}
    \toprule
    Argument & Effect & Exact & Heuristic & Approximation\\
    \cmidrule(lr){1-5}
    -h & prints a help dialog & \check & \check & \check \\
    %
    -s <seed> & seeds the RNG & \check & \check & \check\\
    % 
    -parallel & enables parallel processing (preprocessing streams will
    be parallel, i.\,e., connected components are handled in parallel)
    & \check & \check &
    \check\\
    % 
    -instant & heuristic will output a solution as fast as possible &
    & \check & \\
    %
    -log & prints log information during computation & \check & \check
    & \check \\
    \bottomrule
  \end{longtable}
\end{center}
Some of the arguments may only make sense if \Jdrasil\ is used as
stand alone program, others may also be useful in an scenario where
\Jdrasil\ is used as a library. For instance, the argument ``-parallel''
modifes some algorithms to run in parallel, this is useful in other
applications as well. \Jdrasil\ handles such properties in the class
\JClass{JdrasilProperties} and to enable an property ``-x'' (for
instance ``-parallel'') we can do from source:
\begin{lstlisting}[language=Java]
  // set value of property "x" to "some value"
  JdrasilProperties.setProperty("x", "some value");
  // just add property "parallel"
  JdrasilProperties.setProperty("parallel", "");
\end{lstlisting}
Note that some properties (as the random seed) need an actual value,
this is the second argument in the example code above.
\section{Build \Jdrasil\ from Source}
\Jdrasil\ uses Gradle\footnote{\url{www.gradle.org}} as building
tool. Thereby it takes advantage of the Gradle wrapper: 
in order to build \Jdrasil\ the only requirements are an up-to-date
\texttt{JDK}\footnote{\Jdrasil\ needs at least Java $8$.} and an active internet connection. The Gradle wrapper
will download everything needed by itself.

To get started, we need the latest version of \Jdrasil\ and switch to
its root directory:
\begin{lstlisting}[language=bash]
  git clone https://github.com/maxbannach/Jdrasil.git
  cd Jdrasil
\end{lstlisting}
The folder contains a directory \texttt{subprojects}, which contains
the source code of \Jdrasil. Besides this, there are a couple of Gradle
files, most of which do not require our attention. The only important
file is \lstinline{gradlew} (or \lstinline{gradlew.bat} on
Windows). This is the Gradle wrapper that we will use to build \Jdrasil.
In order to use it, we have to execute on of the following commands
(depending on our operating system):
\begin{lstlisting}[language=bash]
  # on Unix
  ./gradlew <task>
  # or, if gradlew is not executable
  sh gradlew <task>
  # on Windows
  gradlew <task>
\end{lstlisting}
In the rest of the manual, we will use the syntax for an Unix
system. But everything can be done in the same way on a Windows machine.

\subsection{Build the Executable and Library}
To compile the core source code of \Jdrasil\ we use the following
command:
\begin{lstlisting}[language=bash]
  ./gradlew build
\end{lstlisting}
This will create a directory \texttt{build}, containing the
directories \texttt{classes/main} and \texttt{jars}. The first
directory will contain the compiled class files, the second will
contain \texttt{Jdrasil.jar}.

In order to run the freshly build \Jdrasil, we can do one of the
following:
\begin{lstlisting}[language=bash]
  java -jar build/jars/Jdrasil.jar
  java -cp build/jars/Jdrasil.jar jdrasil.Exact
  java -cp build/classes/main jdrasil.Exact
\end{lstlisting}
To use the present build of \Jdrasil\ as library, we can simply add
the created \texttt{.jar} file to the classpath of our desired
project.
\section{Build the Documentation}
The documentation of \Jdrasil\ consists of two parts: the classic
JavaDocs, which provide detailed information about the individual
classes, and this manual, which provides an high level view on some
design principles used by \Jdrasil. The JavaDocs can, without further
requirements, be build by the following command:
\begin{lstlisting}[language=bash]
  ./gradlew javadoc
\end{lstlisting}
This will create the folder \texttt{builds/docs/javadoc} containing
the documentation.

This manual is written in \LaTeX\ and in order to compile it, an
up-to-date \LuaLaTeX\ installation must be available. If this is the
case, the manual can be typeset by
\begin{lstlisting}[language=bash]
  ./gradlew manual
\end{lstlisting}
This command will place this manual as \texttt{.pdf} file in
\texttt{builds/docs/manual}.
\section{Installing Upgrades}
As mentioned earlier, \Jdrasil\ can be build and used without any
dependencies. This makes it easy to update, distribute, and use
\Jdrasil. However, sometimes third-party software may provide a
significant speed up in the process of solving some subproblems. In
such scenarios, the speed of \Jdrasil\ can be improved by
\emph{upgrades}. This topic will be discussed in detail in
section~\ref{part:upgrades}.

All upgrades have in common, that \Jdrasil\ uses the following
convention to build and use them. To $\{\text{download}, \text{build},
\text{install}\}$ (depending on the upgrade) an upgrade $x$, we can
execute the following command:
\begin{lstlisting}[language=bash]
  ./gradlew upgrade_x
\end{lstlisting}
Which will $\{\text{download}, \text{build},
\text{install}\}$ the required files and place them in
\texttt{build/upgrades}.
To run \Jdrasil\ with the installed upgrade, either do (if the upgrade is
a Java library):
\begin{lstlisting}[language=bash]
  java -cp build/jars/Jdrasil.jar:build/upgrades/x.jar  jdrasil.Exact
\end{lstlisting}
or (if the upgrade is a native library):
\begin{lstlisting}[language=bash]
  java -Djava.library.path=build/upgrades -jar build/jars/Jdrasil.jar
\end{lstlisting}
The start scripts of \Jdrasil\ (see next section) will automatically look for upgrades in
these locations.
\section{Building Startscripts (not only ) for PACE}
As \Jdrasil\ was developed for the \emph{Parameterized Algorithms and
  Computational Experiments Challenge} (PACE) \cite{pace} in the first place, it
naturally provides the interfaces required by PACE. To build them, we
can simply use:
\begin{lstlisting}[language=bash]
  ./gradlew pace
\end{lstlisting}
This will, if not already done, build the Java files and will place
two shell scripts in the root directory: \texttt{tw-exact} and
\texttt{tw-heuristic}. The first script will execute \Jdrasil\ in
exact mode (meaning it reads an graph from stdin, computes an optimal
decomposition, and prints it on stdout); while the second script runs
\Jdrasil\ in heuristic mode, meaning it will run in an infinite loop
trying to heuristically find a good decomposition~–~the decomposition
will be printed if a SIGINT or SIGTERM is received. Example usage is:
\begin{lstlisting}[language=bash]
  ./tw-exact -s 42 < myGraph.gr > myGraph.td
  ./tw-heuristic -s 42 < myHugeGraph.gr > myHugeGraph.gr.td
\end{lstlisting}
For more details about the usage of these scripts, take a look at the
PACE website: \url{https://pacechallenge.wordpress.com/pace-2017/track-a-treewidth/}.

These scripts are available both, as Shell script for UNIX systems and
as \texttt{.bat} script for Windows. They can also be build directly
via:
\begin{lstlisting}[language=bash]
  ./gradlew exact
  ./gradlew heuristic
\end{lstlisting}
Finally, there are also scripts available for running \Jdrasil\ with
approximation algorithms:
\begin{lstlisting}[language=bash]
  ./gradlew approximation
\end{lstlisting}
This will generate \texttt{tw-approximation}, which can be used as the
scripts from above.

\part{Graphs}
Since \Jdrasil\ is a tool for computing tree decompositions, it
implements a variety of graph algorithms. Hence, many graphs and
``graph objects'' will be handed throughout the library. It is,
therefore, worth to spend some time and study the design principles
\Jdrasil\ follows when it handles graphs.

All classes and methods that are designed to deal with graphs directly
are stored in the package \JClass{jdrasil.graph}. One (not that small)
exception is the collection of algorithms and procedures that are
meant to compute tree decompositions~–~as this is \Jdrasil's main
task, they obtain some extra packages. Within the graph package, the
working horse is the class \JClass{Graph} which represents all graphs
that occur within \Jdrasil. The following sections will capture the
design of this class, how we can obtain and store objects of the
class, and how we can modify existing \JClass{Graph}
objects. Furthermore, we will discuss how \Jdrasil\ implements
algorithms for computing graph properties and invariants, and, most
importantly, how tree decompositions are managed.

\chapter{The Graph Class}
The work horse of \Jdrasil's graph engine is the class
\JClass{jdrasil.graph.Graph}. There are a couple of design decisions
that were made during the development of this class, which grant some
advantages in the context of computing tree decompositions, but which
may result in disadvantages in other algorithmic tasks. We will
discuss these implementation details in the following. 

An object of the \JClass{Graph} class represents a directed graph
$G=(V, E)$ where $V$ is a set of \emph{arbitrary objects}, and
$E\subseteq V\times V$. An undirected graph is represented as directed
graph with a symmetric edge relation. Since, in the context of
computing tree decompositions, we will often deal with undirected
graphs, the class provides most methods in an additional symmetric
implementation, so that the \JClass{Graph} class can be used to work
with undirected graphs in a natural way.

As stated above, 
\note{This generic approach nevertheless allows the usage of vertex
  classes. Indeed, even the vertex class from, say another library,
  can be plugged in.}
the \JClass{Graph} represents a directed graph where
$V$ is a set of arbitrary objects.  Consequently, there is no vertex
or node class in \Jdrasil, instead, the \JClass{Graph} class is
generic and \emph{any class} can be used as vertex type~–~the only
restriction is that the class has a natural order, i.\,e., it is
\emph{comparable}. For instance, the following graphs are of the types
\JClass{Integer} and \JClass{Character}, respectively.
\begin{center}
\begin{tikzpicture}

\graph[spring electrical layout, edges = {-latex, semithick}, node distance=0.75cm] {
  1 -> {2,3,4};
  2 -> 3;
  4 -> {5, 6 -> 1};
};

\graph[spring electrical layout, edges = {-latex, semithick}, node
distance=0.75cm, anchor at = {(5,0)}] {
  A -> {B,C,D};
  B -> C;
  D -> {E, F -> A};
};
\end{tikzpicture}
\end{center}
We could create the graph from above with any other comparable Java
class; and also run all the algorithms implemented by \Jdrasil\ on
it. Especially, for a class representing subsets of vertices, we can
represent the following graphs:
\begin{center}
\begin{tikzpicture}

\graph[spring electrical layout, edges = {semithick}, node distance=1.5cm] {
  x / "$\{\,1,2,3\,\}$";
  y / "$\{\,1,4,6\,\}$";
  z / "$\{\,4,5\,\}$";
  x --[orient=0] y -- z;
};

\graph[spring electrical layout, edges = {semithick}, node
distance=1.5cm, anchor at = {(5.5,0)}] {
  x / "$\{$\,A,B,C\,$\}$";
  y / "$\{$\,A,D,F\,$\}$";
  z / "$\{$\,D,E\,$\}$";
  x --[orient=0] y -- z;
};
\end{tikzpicture}
\end{center}
This is essentially the way \Jdrasil\ represents tree decompositions,
see Chapter~\ref{chapter:treedecompositions} for more details.

The second part we mentioned earlier is that we simply consider $E$ as
$E\subseteq V\times V$, and in fact, \Jdrasil\ does not have a class
representing an edge. The \JClass{Graph} class only represents
adjacency information about its stored vertices. This makes working
with the \JClass{Graph} class more natural in many situations and
keeps algorithms simple. In the few cases where an edge class would be
useful, for instance if we work with weighted graphs, we interpreted
the edge label function, say $\lambda\colon E\rightarrow\Sigma$, as
$\lambda\colon V\times V\rightarrow\Sigma\cup\{\bot\}$, where we have
$\lambda(u,v)=\bot\Leftrightarrow (u,v)\not\in E$.

\section{Implementation Details}
In the core of the \JClass{Graph} class, the graph is stored as
adjacency list. Hence, we may iterate over the vertex and edge set in
time $O(|V|+|E|)$. Furthermore, the edge relation is stored in an hash
map, allowing us to perform adjacency tests in $O(1)$.

Adding vertices to the graph is performed in $O(1)$ as well, adding an
edge $(u,v)$ costs $O(\delta(v))$\footnote{$\delta(v)$ denotes the
  \emph{degree} of $v$: the number of nodes connected to $v$.}.  Adding directed edges actually is
(practically) faster, as this realized by array manipulation. When an
undirected edge is added, however, this time bound is strict, as some
additional information about the neighborhood is gathered. This
information can be used to get $O(1)$ access to some vertex
properties, for instance, testing if a vertex is simplicial, or
computing the fill-in value of a vertex.

\section{Working with Graphs}
To work with graphs, we first of all need one. Objects of the class
\JClass{Graph} are generated by the \JClass{GraphFactory}~–~a simple
empty graph can be obtained by:
\begin{lstlisting}[language=Java]
  Graph<Integer> myGraph = GraphFactory.emptyGraph();
\end{lstlisting}
Vertices can be added by the method \JMethod{Graph.addVertex}, or by
directly adding edges. The following two code fragments are
equivalent:
\begin{lstlisting}[language=Java]
  // variant 1
  myGraph.addVertex(1);
  myGraph.addVertex(2);
  myGraph.addEdge(1, 2);

  // or simply
  myGraph.addEdge(1, 2);
\end{lstlisting}
The\note{Since \Jdrasil\ mainly works on undirected graphs, methods
  for directed graphs are marked with \emph{directed}, while the
  methods for undirected graph have no prefix.} above code constructs the \emph{undirected} graph
\tikz[baseline={([yshift=-.5ex]1.center)}]\graph[edges={semithick}]{1--2};;
to create the \emph{directed} graph
\tikz[baseline={([yshift=-.5ex]1.center)}]\graph[edges={-latex,
  semithick}]{1->2};, we can use the function \JMethod{Graph.addDirectedEdge}:
\begin{lstlisting}[language=Java]
  // directed version
  myGraph.addDirectedEdge(1, 2);
\end{lstlisting}
We can also mix the commands and create a mixed graph. However,
usually, and in the following, we will stick to undirected graphs.

As we have added them, we can also remove vertices and edges from the
graph:
\begin{lstlisting}[language=Java]
  // removes the vertex and all incident edges, i.e., the one from 1 to 2
  myGraph.removeVertex(1);

  // removes the specific undirected edge
  myGraph.removeEdge(2, 3);
\end{lstlisting}
Removing an undirected edge will remove both directed edges, even if
only one of them is present (i.\,e., if the edge is actually
directed). Concretely, deleting a directed edge can be done by:
\begin{lstlisting}[language=Java]
  // directed version
  myGraph.removeDirectedEdge(1, 2);
\end{lstlisting}
\subsection{Iterating Over the Graph}
Many graph algorithms have to iterate over the vertices and edges of
the graph. The \JClass{Graph} implements an iterator for its vertices,
so to iterate over the vertices we can simply do (here, the graph has
vertices of type \JClass{Integer}):
\begin{lstlisting}[language=Java]
  for (Integer v : myGraph) {
    // do something with v
  }
\end{lstlisting}
As \Jdrasil\ does not know edge objects, there is no direct iteration
over edges. Instead, the neighborhood of a vertex is iterable as well.
In the directed case, this is straight forward:
\begin{lstlisting}[language=Java]
  for (Integer v : myGraph) {
    for (Integer w : myGraph.getNeighborhood(v)) {
      // do something with the edge (v,w)
    }
  }
\end{lstlisting}
In an undirected graph, however, we would iterate over every edge
twice (remember that an undirected graph is an directed graph with
symmetric edge relation). To overcome this issue, we can only pick the edge
in which the first vertex is lexicographical smaller:
\begin{lstlisting}[language=Java]
  for (Integer v : myGraph) {
    for (Integer w : myGraph.getNeighborhood(v)) {
      if (v.compareTo(w) > 0) continue; // skip second edge
      // do something with the undirected edge {v,w}
    }
  }
\end{lstlisting}
\subsection{Some Special Methods}
While most parts of the \JClass{Graph} class are kept as general as
possible, there are some special methods, which are designed towards
the computation of tree decompositions. They are defined for
\textcolor{jdrasil.fg}{undirected graphs} only, and will produce
mixed graphs or undefined results on directed graphs. These methods
are:
\begin{enumerate}
  \item\lstinline[language=Java]{contract(T v, T w)}: Contracts the
    \emph{undirected} edge $\{v,w\}$ into the vertex $v$. This will
    connect all edges incident to $w$ to $v$~–~this will, however,
    \emph{not} create multi-edges. This method will return a
    \JClass{ContractionInformation} object, which can be used to
    revert the contraction.
 \item\lstinline[language=Java]{deContract(ContractionInformation info)}: Will revert the contraction of an edge.
  \item\lstinline[language=Java]{eliminateVertex(T v)}: Eliminating
    a vertex $v$ will a) turn $N(v)$\footnote{$N(v)$ denotes the
      \emph{neighbourhood} of $v$: all nodes connected to $v$.} into a clique, and b) remove $v$
    from the graph. This method will return an
    \JClass{EliminationInformation} object, which can be used to
    revert the elimination.
  \item\lstinline[language=Java]{deEliminateVertex(EliminationInformation info)}:
 Will revert the elimination of a vertex.
 \item\lstinline[language=Java]{getSimplicialVertex(Set<T> forbidden)}: Get an arbitrary simplicial vertex, that is not
   contained in the forbidden set. A simplicial vertex is a vertex that has a
   clique as neighborhood. As required information are already
   gathered during the construction of the graph, this method runs in
   $O(|V|)$.
 \item\lstinline[language=Java]{getAlmostSimplicialVertex(Set<T> forbidden)}: As the last method, but will return an \emph{almost}
   simplicial vertex, that is, a vertex that has a clique and one other
   vertex as neighborhood. This method actually computes the vertex
   and is more expensive than the last one.
 \item\lstinline[language=Java]{getFillInValue(T v)}: The fill-in
   value of an vertex $v$ is the amount of edges that will be
   introduced to the graph, if $v$ is eliminated. This method runs in
   $O(1)$ as well.
\end{enumerate}

\section{Reading and Writing Graphs}
In the previous section we have created a graph ``by hand'', in real
scenarios, however, we will often need to read the graph from
standard input or from a file. The class \JClass{GraphFactory}
provides a method to read \texttt{.gr} files (which is the graph format
specified by
PACE\footnote{\url{https://pacechallenge.wordpress.com/pace-2016/track-a-treewidth/}}. These
methods are quite generic, and also work for \texttt{DIMACS} (graph)
files, i.\,e., \texttt{.dgf} files. A typical usage would be:
\begin{lstlisting}[language=Java]
  Graph<Integer> myGraph = GraphFactory.graphFromStdin();
\end{lstlisting}
A graph, however, can not only be created from a stream. A common
source for a graph is, well, another graph. The \JClass{GraphFactory} class
provides methods to copy graphs:
\begin{lstlisting}[language=Java]
  Graph<Integer> myGraph = GraphFactory.copy(othergraph);
\end{lstlisting}
Another way to obtain a graph from a graph is by taking a subgraph,
that is, a graph defined by a subset of the vertices. In \Jdrasil,
this can be achieved by the following code:
\begin{lstlisting}[language=Java]
  Set<Integer> subgraph = new HashSet<>();
  // ... add vertices to subgraph ...
  Graph<Integer> myGraph = 
    GraphFactory.graphFromSubgraph(othergraph, subgraph);
\end{lstlisting}

Once \Jdrasil\ did its job, we most likely want to print the graph or
its tree decomposition to the standard output or a file. This can be
achieved with the class \JClass{GraphWriter}, which provides many
methods to print graphs. To simply write a graph, or a tree
decomposition, to the standard output, we can use the following code:
\begin{lstlisting}[language=Java]
  // write a graph of any type
  GraphWriter.writeGraph(myGraph);

  // write a graph of any type, translate vertices to {1,...,|V|}
  GraphWriter.writeValidGraph(myGraph);

  // write tree decomposition
  GraphWriter.writeTreeDecomposition(decomposition);
\end{lstlisting}
Additionally, \Jdrasil\ also provides methods to write graphs and tree
decomposition directly into Ti\emph{k}Z code, which is very useful for
debugging, especially combined with the \emph{graph drawing}
capabilities of Ti\emph{k}Z:
\begin{lstlisting}[language=Java]
  // write a graph to TikZ
  GraphWriter.writeTikz(myGraph);

  // write tree decomposition to TikZ
  GraphWriter.writeTreeDecompositionTikZ(decomposition);
\end{lstlisting}

\chapter{Graph Invariants}
A graph property is a class of graphs that is closed under
isomorphisms. A graph invariant is a function that maps isomorphic
graphs to the same value. Examples for graph invariants are ``number of
vertices'', or ``size of the minimum vertex-cover''. An example of a
graph property could be ``contains a triangle''. In this sense, we can
model graph properties as invariants that map to boolean values.

\Jdrasil\ implements graph invariants over the interface \JClass{Invariant}. In order to do so, it
essentially provides the method \JMethod{getValue}, which returns the
computed invariant.  Since many invariants can be represented by an
additional model (for instance, a model for the vertex-cover model
is the actual vertex-cover), the class also provides the method
\JMethod{getModel}, which returns a map from vertices to some values.

The usual usage of an invariant is as follows (here, we use
vertex-cover):
\begin{lstlisting}[language=Java]
  // this will already compute the vertex cover
  VertexCover vc = new VertexCover<T>(myGraph);

  // the following methods then cost O(1)
  vc.getValue(); // size of the vertex-cover
  vc.getModel(); // the vertex-cover

  // since some invariants are hard to compute, we may not 
  // obtain an optimal solution, we can check as follows
  vc.isExact()
\end{lstlisting}

\section{Connected Components}
The class \JClass{ConnectedComponents} can be used to compute the
connected components of the graph. The model maps vertices to
integers, which represent the id of the connected component the vertex
is. In addition, the class provides methods to obtain the connected
components as sets of vertices and as subgraphs.

\section{Vertex Cover}
The class \JClass{VertexCover} computes a set of vertices that covers all edges. If an SAT solver is
available, this class will compute a minimal vertex-cover. Otherwise,
a 2-approximation is used.

\section{Clique}
The class \JClass{Clique} computes a set of vertices that are all pairwise adjacent. If an SAT
solver is available, a maximal clique will be computed. Otherwise, a
greedy strategy is used.

\section{Twin Decomposition}
The class \JClass{TwinDecomposition} computes a twin decomposition of
the graph. Two vertices $u$ and $v$ are
called twins if we have $N(u)=N(v)$. This relation defines an
equivalence relation on the graph, which we call twin decomposition.

\section{Minimal Separator}
The class \JClass{MinimalSeparator} computes a minimal separator, i.\,e., a minimal set of vertices such
that the removal of the set will increase the number of connected
components of the graph.

\section{Maximal Matching}
A matching in a graph \(G=(V,E)\) is a subset of the edges
\(M\subseteq E\) such that for every vertex \(v\in V\) we have
\(|\{\,e\mid e\in M\wedge v\in e\,\}|\leq 1\). A matching is \emph{maximal} if
we can not increase it by adding any edge. It is a \emph{maximum} matching,
if there is no bigger matching in the graph, and it is \emph{perfect} if
every vertex is matched.
  
In \Jdrasil, the class \JClass{Matching} greedily computes a maximal matching, i.\,e., it is not
guaranteed that it is a maximum matching. The matching is represented
as map from vertices to vertices, i.\,e., a vertex is mapped to the
vertex it is matched with.
  
\chapter{Tree Decompositions}\label{chapter:treedecompositions}
Since \Jdrasil{} is a library for computing tree decompositions, a
well thought through representation of such decompositions is
required. Recall the formal definition of a tree decomposition: A
\emph{tree decomposition} of a graph $G$ is a pair $(T,\iota)$
consists of a tree $T$ and a mapping $\iota$ from nodes of $T$ to
subsets of vertices of $G$ (called \emph{bags}), such that:
\begin{itemize}
  \item $\forall x\in V(G)$ there is a node $n\in V(T)$ with $x\in\iota(n)$;
  \item $\forall \{x,y\}\in E(G)$ there is a node $n\in V(T)$ with $\{x,y\}\subseteq\iota(n)$;
  \item $\forall x,y,z\in V(T)$ we have $\iota(y)\subseteq \iota(x)\cap\iota(z)$
whenever $y$ lies on the unique path between $x$ and $z$ in $T$.
\end{itemize}

\section{Design Principle}
The formal definition of tree decompositions and the way \Jdrasil\
represents graphs (remember that graphs can build up on any vertex
class) gives rise to the following approach: a tree decomposition in
\Jdrasil\ is pretty much just a \JClass{Graph} with a special vertex
class \JClass{Bag}. The \JClass{Bag} class on the other hand is actually just a
collection of vertices.

To put it all together, the class \JClass{TreeDecomposition}
represents a tree decomposition. It can store the original graph $G$,
and a graph representing the tree $T$ with \JClass{Bag}
vertices. Furthermore, the class \JClass{TreeDecomposition} provides
methods to create new nodes in the decomposition and to link existing nodes.

\section{Building and Using Tree Decompositions}
Let us assume we wish to implement a new algorithm for computing a
tree decomposition. The following code snippets illustrate how we would
create and store the decomposition in \Jdrasil. Note that during the
construction, the tree decomposition has not to be valid in any means.

Assume we read a graph with integer vertices from the standard
input. We can create a tree decomposition for the graph as
follows. At this point, the decomposition will be \emph{empty} and
\emph{invalid}, i.\,e., it will only correspond to the graph, but will
not have any information about the decomposition stored yet.
\begin{lstlisting}[language=Java]
  // read the graph G
  Graph<Integer> G = GraphFactory.graphFromStdin();
  // create empty tree decomposition for graph G
  TreeDecomposition<Integer> td = new TreeDecomposition<>(G);

  // check if the decomposition is valid
  boolean isValid = td.isValid(); // returns false
\end{lstlisting}
To create a bag, we need a subset of the vertices of $G$ that we wish
to store in the bag. Finding these subsets is of course the (hard) task of
the algorithm. Let us, in this example, just create the trivial tree
decomposition of a single bag:
\begin{lstlisting}[language=Java]
  Set<Integer> someVertices = G.getVertices();
  Bag<Integer> myBag = td.createBag(someVertices);

  // decomposition now is valid
  boolean isValid = td.isValid(); // returns true
\end{lstlisting}
The bag is created with the method \JMethod{createBag}. This will do
two things: it will create the \JClass{Bag} object and store it in the
tree decomposition, and it will return a reference to this
\JClass{Bag} object as well. This reference may be needed later on,
for instance, if we wish to connect some bags with a tree edge:
\begin{lstlisting}[language=Java]
  // anotherBag is a reference to another bag that we have created
  td.addTreeEdge(myBag, anotherBag);
\end{lstlisting}
And that is it. We have successfully created a tree
decomposition. \Jdrasil\ allows us to improve the quality of the
decomposition (whenever we know that we do not have an optimal
decomposition) with the separator technique
of~\cite{bodlaender2010treewidth}. Just do the following and the
decomposition may be improved:
\begin{lstlisting}[language=Java]
  td.improveDecomposition();
\end{lstlisting}
This method, however, may take some time if there are huge bags and
should therefore be used on already good decompositions (and not on the
trivial one as in this example).

\part{Algorithms}
At its core engine, \Jdrasil\ implements a bunch of algorithms to
compute tree decompositions. These algorithms either directly compute
such decompositions, or assist other algorithms in doing so. In
particular, the implemented algorithms (that are related to the
computation of tree decompositions) are partitioned into five types:
\emph{Preprocessing} algorithms, algorithms that compute
\emph{lowerbounds}, \emph{heuristics}/\emph{upperbounds}, \emph{approximation}
algorithms, and \emph{exact} algorithms.

\chapter{Preprocessing}
A Preprocessor is a function that maps an arbitrary input graph to a collection of ``easier'' graphs. 
Easier here is meant with respect to computing a tree decomposition and often just means ``smaller'', but 
could also refer to adding structures to the graph that improve pruning potential.
 
The class \JClass{Preprocessor} models a preprocessor by providing the
methods \JMethod{computeGraphs}, \JMethod{addbackTreeDecomposition},
\JMethod{glueDecompositions}, and \JMethod{getTreeDecomposition}. The
first method represents the actual preprocessing and computes a
collection of graphs from the input graph. The following two methods
can be used to add a tree decomposition of one of the produced graphs
back, and to combine these tree decompositions to one for the input
graph. The last method is a getter for this decomposition.

The usual way to use a preprocessing algorithm is by a) initializing an
instance of it, b) loop over the generated graphs, and c) add back a
tree decomposition for every graph. For instance, if we wish to
iterate over safe components (connected components, bi-connected
components, etc.) we can do the following:
\begin{lstlisting}[language=Java]
  
  // a) generate instance of preprocessing algorithm
  GraphSplitter<T> splitter = new GraphSplitter<>(graph);

  // b) iterate over all generated graphs
  for (Graph<T> component : splitter) {
    // c) compute decomposition of the component
    TreeDecomposition<T> decomposition = ... 
    // and add it to the preprocessor object
    splitter.addbackTreeDecomposition(decomposition);
  }

  // we can now access the final decomposition of the original graph
  splitter.getTreeDecomposition();
\end{lstlisting}

The \JClass{Preprocessor} class also supports the Stream API of Java
8, i.\,e., the above example can also be formulated as follows:
\begin{lstlisting}[language=Java]
  
  // a) generate instance of preprocessing algorithm
  GraphSplitter<T> splitter = new GraphSplitter<>(graph);

  // b) iterate over all generated graphs (may be parallel!)
  splitter.stream().forEach( component -> {
    // c) compute decomposition of the component
    TreeDecomposition<T> decomposition = ... 
    // and add it to the preprocessor object
    splitter.addbackTreeDecomposition(decomposition);
  });


  // we can now access the final decomposition of the original graph
  splitter.getTreeDecomposition();
\end{lstlisting}
As the streaming interface allows easy parallelization, this is
supported by \Jdrasil\ as well. In the above code, each component will
be handled in parallel if the ``parallel'' flag is set in the
configuration class \JClass{JdrasilProperties}. This is usually done
by a program argument.

\section{Reduction Rules}
There are many reduction rules for tree width known in the
literature, see for instance~\cite{DowneyF2013}. A reduction rule thereby is a function that removes (in
polynomial time) a vertex from the graph and creates a bag of the
tree decomposition such that this bag can be glued to an optimal
tree decomposition of the remaining graph~–~yielding  an optimal
tree decomposition. For graphs of tree width at most 3, these rules
produce an optimal decomposition in polynomial time.  For graphs with
higher tree width, the rules can only be applied up to a certain
point. From this point on, another algorithm has to be used.

In \Jdrasil, the class \JClass{GraphReducer} implements several
reduction rules and can be used as preprocessing for any other
algorithm. As mentioned before, this class will (automatically)
compute optimal tree decompositions of graphs with tree width at most
$3$. If this class fully reduces the input graph, it will generate an
empty set of graphs. So it is always save to simply loop over the
generated graphs, without caring about this special case.

\section{Splitting up the Graph}
A separator $S\subseteq V$ of a graph $G=(V,E)$ is a subset of the
vertices such that $G\setminus S$ has more connected components
than $G$. In particular, the connected components of $G$ are separated
by the separator $\emptyset$, while bi-connected components have a
separator $S$ with $|S|=1$.

Bodlaender and Koster have presented a list of \emph{safe} separators
for tree width~\cite{BodlaenderK2006}. A safe separator is one that
does not effect the tree width, i.\,e., one that allows to reproduce
an optimal tree decomposition for $G$ from optimal tree decompositions
of the connected components of $G\setminus S$ (in polynomial
time). Having such safe separators allows us to compute tree
decomposition in an divide-and-conquer manner: Split the graph using
safe separators until the graph is small enough to solve it, or until
there are no separators left.

In \Jdrasil, the class \JClass{GraphSplitter} implements some safe
separators. The class will split the graph using such separators, and
will keep track of potential glue points. Once the class is provided
with tree decompositions for all components produced, it will generate
a tree decomposition for the original graph.

\section{Contracting the Graph}
It is a well known fact that tree width is closed under taking
minors\footnote{A \emph{minor} of $G$ can be formed by deleting edges
  and vertices and contracting edges.},
i.\,e., if \(H\) is a minor of \(G\), then \(\mathrm{tw}(H)\leq
\mathrm{tw}(G)\). Many algorithms that compute tree decompositions use
this fact in some way.
  
The \JClass{GraphContractor} class computes a matching of the input graph, and
contracts it. The result is a minor (which is of course much smaller)
that is returned. Given a tree decomposition of this minor, a tree
decomposition for the original graph is generated by decontracting the
edges within the bags of the decomposition. The result is a valid (but
not optimal) tree decomposition of the input graph. Furthermore, if
the decomposition of the minor has width \(k\), the width of the final
decomposition is at most \(2k+1\).
  

\chapter{Lowerbounds}
The \JClass{lowerbound} interface describes a class that models an
algorithm for computing a lower bound of the tree width of a graph.
By implementing this interface, a class has to implement the Callable
interface, which computes a lower bound on the tree width 
a graph. In addition, a class implementing this interface has to
implement the method \JMethod{getCurrentSolution}, which can be used
if the lower bound algorithm can already provide lower bounds during
its execution. \Jdrasil{} implements the following lowerbound algorithms.

\section{Degeneracy of a Graph}
We call a Graph $G=(V,E)$ $d$-degenerated if each subgraph $H$ of $G$ contains
a vertex of maximal degree $d$.  It is a well known fact that we have
\(d \le tw(G)\) and, thus, we can use the degeneracy of a graph as
lowerbound for the tree width.

The class \JClass{DegeneracyLowerbound} implements the linear time
algorithm from Matula and Beck~\cite{MatulaB1983} to compute the
degeneracy of a graph.

\section{Lowerbounds based on Minors}
It is a well known fact that for every minor $H$ of $G$ the following
holds: \(tw(H) \le tw(G)\). To obtain a lowerbound on the tree width
of $G$ it is thus sufficient to find good lowerbounds for minors of
$G$.  The minor-min-width heuristic devoloped by Gogate and Dechter
for the QuickBB algorithm~\cite{GogateD2004} does exactly this. It
computes a lowerbound for a minor of G and tries heuristically to find
a good minor for this task.

In \Jdrasil{} the class \JClass{MinorMinWidthLowerbound} implements
this heuristic with different strategies discussed by Bodlaender and Koster~\cite{bodleanderK2011}.

\section{Compute Lowerbound in Improved Graphs}
The \{$k$-neighbor, $k$-path\}-improved graph \(H\) of a given graph \(G\)
is obtained by adding an edge between all non adjacent vertices that
have at least $k$ \{common neighbors, vertex disjoint paths\}. A crucial
lemma states that adding these edges will not increase the tree
width. Hence, we can compute a lower bound on the tree width of \(G\)
by computing lower bounds on increasing improved graphs of \(G\). In
this way, improved graphs can be used to improve the performance of
any lower bound algorithm.

The class \JClass{ImprovedGraphLowerbound} implements the improved graph trick to improve some
implemented lower bound algorithms. The implementation is based on~\cite{bodleanderK2011}.

\chapter{Heuristics}
Since computing the exact tree width of a graph is
$\Class{NP}$-hard, there are many graphs for which we are not yet
able to compute an optimal tree decomposition. However, it turns out
that there are very powerful heuristics for this problem.

In \Jdrasil{}, all heuristics implement the interface
\JClass{TreeDecomposer} and produce tree decompositions of quality ``Heuristic''.

In addition to the different implemented algorithms, there is the
class \JClass{Heuristic} which provides stand alone access to a
combination of heuristics and is used by us for the PACE heuristic
track. This class can be compiled and used with
\begin{lstlisting}[language=bash]
  ./gradlew heuristic
  ./tw-heuristic
\end{lstlisting}
The program combines some of the implemented heuristics to get the
best from each world. In particular, it does the following:
\begin{enumerate}
  \item reduce the graph using \JClass{GraphReducer};
  \item computes a first decomposition using;
    \JClass{StochasticGreedyPermutationDecomposer};
  \item tries to improve the decomposition;
  \item starts an infinite local search to further improve the
    decomposition using the algorithm implemented in \JClass{LocalSearchDecomposer}.
\end{enumerate}
The program will try to improve the decomposition, until a
\texttt{SIGTERM} is received. 

\section{Greedily Compute an Elimination Order}
The class \JClass{GreedyPermutationDecomposer} implements greedy
permutation heuristics to compute a tree decomposition. The heuristic
eliminates the vertex $v$ that minimizes some function $\gamma(v)$,
while ties are broken randomly. See~\cite{bodlaender2010treewidth}
for an overview of possible functions $\gamma$. The class implements
six different value functions, which can be selected by the method
\JClass{setToRun}. Experiments have shown that different functions are
preferable on different graphs. It is, thus, worth to test different
value function when we compute a tree decomposition of a graph.

To improve the quality of the found decomposition, we can do some sort
of \emph{look-ahead}: instead of taking the vertex that minimizes
$\gamma$, we take the vertex such that the sum of the next $k$ choices
minimizes $\gamma$. A look-ahead for $k>1$ can be set with
\JClass{setLookAhead}. Already for $k=2$ this improves the quality of
the heuristic on many graphs. However, increasing $k$ by one increases
the running time by a factor of $|V|$ and, hence, the look-ahead
should be used with care.

\section{Greedily Compute an Elimination Order with Many Coins}
The Greedy-Permutation heuristic performs very well and can be seen as
\emph{randomized algorithm} as it breaks ties randomly.  Therefore,
multiple runs of the algorithm produce different results and, hence,
we can perform a stochastic search by using the heuristic multiple
times and by reporting the best result. As the
Greedy-Permutation heuristic implements different algorithms, we can
pick different algorithms in different runs. As the performance of
these algortihms differ, we choose them with different probabilities.
In \Jdrasil{}, this strategy is implemented by the class \JClass{StochasticGreedyPermutationDecomposer}.

\section{Maximum Cardinality Search}
The class \JClass{MaximumCardinalitySearchDecomposer} implements the
Maximum-Cardinality Search heuristic. The heuristic orders the vertices
of $G$ from $1$ to $n$ in the following order: We first put a
random vertex $v$ at position $|V|$. Then we choose the vertex $v'$
with the most neighbors that are already placed at position $|V|-1$
and recurse this way. Ties are broken randomly.

\section{Local Search}
The class \JClass{LocalSearchDecomposer} implements a tabu search on
the space of elimination orders developed by Clautiaux, Moukrim,
N{\`e}gre, and Carlier~\cite{clautiaux2004heuristic}.

The algorithm expects two error parameters $r$ and $s$: the number of
restarts and the number of steps.  To find a tree decomposition the
algorithm starts upon a given permutation, then it will try to move a
single vertex to improve the decomposition and do this $s$ times. If
no vertex can be moved, a random vertex is moved and the process
restarts. At most $r$ restarts will be performed.  At the end, the
best found tree decomposition is returned.

\chapter{Approximation}
An approximation algorithm as the one by Robertson and Seymour~\cite{RobertsonS1995} can be interesting in two ways: it
produces lower \emph{and} upper bounds at the same time, while also
provides a guarantee on its quality. This section lists the
approximation algorithms implemented by \Jdrasil.

In \Jdrasil{}, all approximation algorithms implement the interface
\JClass{TreeDecomposer} and produce tree decompositions of quality ``Approximation''.

In addition to the different implemented algorithms, there is the
class \JClass{Approximation} which provides stand alone access to a
combination of approximation algorithms.
\begin{lstlisting}[language=bash]
  ./gradlew approximation
  ./tw-approximation
\end{lstlisting}
The program combines some of the implemented approximations to get the
best from each world. In particular, it does the following:
\begin{enumerate}
  \item split the graph into safe components using \JClass{GraphSplitter}
  \item reduce the graph using \JClass{GraphReducer};
  \item computes a decomposition using \JClass{RobertsonSeymourDecomposer}.
\end{enumerate}

\section{Robertson and Seymour like Approximation}
The class \JClass{RobertsonSeymourDecomposer} uses the standard
$\Class{FPT}$\footnote{$\Class{FPT}$ stands for \emph{fixed-parameter
    tractable}.} approximation algorithm for tree width based on the work of Robertson and Seymour.
The algorithm assumes that the graph is connected and computes in time \(O(8^k k^2 \cdot n^2)\) a tree decomposition
of width at most \(4k+4\).

A detailed explanation of the algorithms can be found in many text
books about $\Class{FPT}$, for instance~\cite{CyganFKLMPPS2015, FlumGrohe2006}.

\chapter{Exact}
At the heart of \Jdrasil{} is a collection of algorithms that compute
optimal tree decompositions. As different algorithms may be preferable
on different graphs, the class \JClass{ExactDecomposer} selects
between different algorithms, in particular it will:
\begin{enumerate}
  \item split the graph into safe components using \JClass{GraphSplitter};
  \item reduce the graph using \JClass{GraphReducer};
  \item solve the problem with \JClass{SATDecomposer} if a SAT solver
    is available;
  \item solve with \JClass{CopsAndRobber}.
\end{enumerate}

In \Jdrasil{}, all exact algorithms implement the interface
\JClass{TreeDecomposer} and produce tree decompositions of quality ``Exact''.

In addition to the different implemented algorithms, there is the
class \JClass{Exact} which provides stand alone access to the
\JClass{ExactDecomposer} class. This class can be compiled and used with
\begin{lstlisting}[language=bash]
  ./gradlew exact
  ./tw-exact
\end{lstlisting}

\section{Branch and Bound}
The class \JClass{BranchAndBoundDecomposer} implements a classical branch and bound algorithm based on QuickBB~\cite{GogateD2004} and its successors.
The algorithm searches through the space of elimination orders and utilizes dynamic programming,
reducing the search space to $O(2^n)$.

\section{Cops and Robber}
A classic result of Seymour and Thomas~\cite{SeymourT1993}
provides a connection of the tree width of the graph and the
cops-and-robber search game. Together with an algorithm by Berarducci
and Intrigila~\cite{BerarducciI1993} to evaluate such games in time
$n^{O(tw(G))}$, this yields a way to compute exact tree decompositions. In
\Jdrasil{} the class \JClass{CopsAndRobber} implements this approach.

\section{Dynamic Programming}
The class \JClass{DynamicProgrammingDecomposer} implements exact exponential time (and exponential space)
algorithms to compute a tree decomposition via dynamic programming.
The algorithms are based on the work of 
Bodlaender, Fomin, Koster, Kratsch, and Thilikos~\cite{BodlaenderFKKT2012}.

\section{Naive Brute Force}
The tree width characterization via elimination order gives a very
simple brute force algorithm: just check all $n!$ permutations. This
is of course only feasible for very small graphs. The approach is
implemented by the class \JClass{BruteForceEliminationOrderDecomposer}.

\section{Using a SAT Solver}
A very common (theoretical and practical) approach to solve intractable
problems is to first represent them as \emph{constraint satisfaction
  problems} (CSP) and then solve those problems via specialized solvers. The
most widely used solvers are SAT solvers that work on Boolean
formulas.

\Jdrasil{} provides the classes \JClass{BaseEncoder} and
\JClass{ImprovedEncoder} to encode the problem of finding an optimal
tree decomposition into a logic formula. The class
\JClass{SATDecomposer} constructs such formulas, solves them using a
SAT solver, and extracts the corresponding tree decomposition.

The class \JClass{SATDecomposer} only works if a SAT solver is
installed as upgrade, see Section~\ref{upgrade:sat} for details.

\part{Upgrades}\label{part:upgrades}

\Jdrasil\ is designed as a modular and platform independent library,
which provides all the advantages discussed earlier, but also comes
with a couple of problems. In particular, in order to compute a tree
decomposition of a graph, \Jdrasil\ internally solves many different
combinatorial optimization problems. Some of these problems may be
solved more efficiently on a specific target platform, rather than on
Javas virtual machine. For instance, one may want to use present
graphic cards for massive parallelization. On the other hand, for many
of these problems there are excellent and optimized libraries
available, which we want to use. For instance, \Jdrasil\ will rather
use existing \Lang{SAT} solvers to solve the boolean satisfiability
problem, instead of implementing its own. The concept of
\emph{upgrades} is \Jdrasil's way to use external code or
libraries. 

We use the term ``upgrade'', in contrast to something like
``library'', as \Jdrasil\ will always be fully functional and platform
independent without any upgrade. In particular, it can be compiled and
shipped without any upgrade. On the other hand, an upgrade will, as
the name suggests, speed up \Jdrasil\ on certain instances or
platforms. In other words, an upgrade will not increase the
functionality of \Jdrasil, but will provide tools for \Jdrasil\
such that it can execute its functionality faster.

The default location of upgrades for \Jdrasil\ is the folder
\file{build/upgrades/}. Since \Jdrasil\ comes
without any upgrade, this folder, at default, does not contain
much. However, the gradle file of \Jdrasil\ provides some targets
to obtain upgrades.
\begin{center}
\vfill
\fbox{
  \begin{minipage}{0.75\textwidth}
  \vspace{1ex}
  ~\hfill\textcolor{jdrasil.alert}{Licence Warning}\hfill~
  \vspace{0.5ex}

  While \Jdrasil\ stands under the open MIT licence, the third party
  software installed through upgrades may not. Whenever we install an
  upgrade, we may have to restrict the licence. Please read the
  licence of used third party software before you install an upgrade.
  \end{minipage}
}
\vfill
\end{center}
\chapter{Boolean Satisfiability}\label{upgrade:sat}
The boolean satisfiablity problem \Lang{SAT} is the most canonical
$\Class{NP}$-complete problem, and ``simply'' asks if a boolean
formula in CNF has a satisfying model. Many $\Class{NP}$-complete
problems can naturally be stated as a \Lang{SAT}-problem, and hence, can
be naturally solved by finding a model for a CNF formula. This is the
reason why \emph{SAT solvers}, i.\,e., tools that solve the boolean
satisfiablity problem, have received a lot of research effort. In
particular, there are annual challenges\footnote{\url{http://baldur.iti.kit.edu/sat-competition-2016/}} that try to
find the fastest solver. The result of this effort is that modern
SAT solver can solve hard problems on many instances very quickly.

The power of SAT solvers makes it interesting to use them while
computing a tree decomposition. Equipped with a SAT solver, \Jdrasil\
can directly encode the problem of finding a tree decomposition (or
more precisely, an elimination order) into a $\Lang{SAT}$-formula. On the other hand, \Jdrasil\ can
also use the SAT solver to solve different subproblems while computing
the tree decomposition. For instance, if \Jdrasil\ computes an
elimination order, it can always put a clique of the graph at the
end of the permutation. If the clique is large, this can reduce the
search space dramatically. However, finding large cliques in a graph is
$\Class{NP}$-hard as well and \Jdrasil\ will thus use a SAT solver to find the
largest clique in the graph.

\section{The Formula Class}\label{section:satFormula}
The main interface of \Jdrasil\ to use boolean logic is the class
\JClass{jdrasil.sat.Formula}, which represents a boolean formula in
CNF. This class is always available and can always be used to create
and manage logic formulas. 

\subsection{Specifying a Formula}
A formula $\phi$ is always represented in CNF and in the classic
DIMACS format, that is, variables are positive integers
$x\in\mathbb{N}$, and negated variables are simply stored as $-x$. We
can specify a formula by adding clauses to it, for instances
$\phi=(x_1\vee\neg x_2\vee x_3)\wedge(x_2\vee\neg x_3)\wedge x_3$ can
be created as follows:

\begin{lstlisting}[language=Java]
  Formula phi = new Formula();
  phi.addClause(1, -2, -3);
  phi.addClause(2, -3);
  phi.addClause(3);
\end{lstlisting}

We can also ``concatenate'' two formulas by combining them with a logic
``and'', i.\,e., we can compute $\phi\wedge\psi$:

\begin{lstlisting}[language=Java]
  Formula psi = new Formula();
  psi.addClause(-1);
  
  phi.and(psi);
\end{lstlisting}

We can always add clauses to an existing formula or concatenate it
with another formula. With other words, we can always further
restrict the solution space of a formula. Sometimes, however, we may
wish to remove a clause, which can be done by:
\begin{lstlisting}[language=Java]
  psi.removeClause(-1);
\end{lstlisting}
But this operation should be used with caution: first of all it is
much more expensive to remove a clause than adding one; and,
furthermore, we are not always allowed to remove a clause (the method
can throw an exception). The reason for this is that SAT solvers that
solve a formula incrementally often only allow to restrict the formula
further. This means, once we started to ``solve'' the formula, we
can not remove clauses anymore.

\subsection{Solving a Formula}
So, \note{We can only solve formulas if a SAT solver is installed as upgrade.}
how to actually find a model for the formula, i.\,e., how to
``solve'' it. In order to check if a formula has a satisfying
assignment, \Jdrasil\ uses external SAT solvers which have to be
installed as upgrade (see the following sections for possible
solvers). If a SAT solver is installed, we can register it to the
formula:
\begin{lstlisting}[language=Java]
  String sig = phi.registerSATSolver();
\end{lstlisting}
This method will register an arbitrary SAT solver that \Jdrasil\ has
found as upgrade. If no SAT solver is installed, this method will
throw an exception; otherwise the signature of the solver is
returned. Once a solver is registered, the following will happen:
\begin{enumerate}
  \item The formula ``as is'' will be transfered to the solver,
    i.\,e., all clauses stored will be send to the solver.
  \item The formula and the solver will be kept in sync, that is,
    clauses added to the formula will directly be added to the solver.
  \item The method \JMethod{removeClause()} can not be called
    anymore.
  \item The method \JMethod{isSatisfiable()} can now be called.
\end{enumerate}

Once a solver was registered to the formula, we can check if there is
a satisfying assignment:
\begin{lstlisting}[language=Java]
  phi.isSatisfiable();
\end{lstlisting}
This method will use the SAT solver to solve the formula. The whole
API is incremental, so we can modify the formula between calls of
this method (which will be faster than recreating new formulas). A
typical scenario would look like:
\begin{lstlisting}[language=Java]
  while (phi.isSatisfiable()) {
    phi.addClause(...);
    ...
  }
\end{lstlisting}
Sometimes, we actually would like to remove clauses between calls
(which we are not allowed to do, as mentioned earlier). To overcome
this issue, most incremental SAT solvers support the concept of
\emph{assumptions}. An assumption is an unit clause that is added to
the solver for a single run. We can for instances say
$x_1=\mathrm{true}$ and check if the formula is satisfiable
\emph{under this assumption}. After a
call of \JMethod{isSatisfiable()}, all assumptions are removed.
To check if a formula is satisfiable under a set of assumptions,
simply add them to the method call:
\begin{lstlisting}[language=Java]
  phi.isSatisfiable(1, -3);
\end{lstlisting}
Once we have defined a formula and solved it using
\JMethod{isSatisfiable()}, we are most likely interested in an actual
satisfying model. A model is a mapping from the variables to boolean
values, i.\,e., a \JClass{Map<Integer, Boolean>} and can be obtained
with the following call:
\begin{lstlisting}[language=Java]
  Map<Integer, Boolean> model = phi.getModel();
  System.out.printf("Value of %d is %b\n", 1, model.get(1));
  System.out.printf("Value of %d is %b\n", 2, model.get(2));
  System.out.printf("Value of %d is %b\n", 3, model.get(3));
\end{lstlisting}
Note that we can only obtain a model after a call to
\JMethod{isSatisfiable()}, and only if this call has returned
true. Otherwise the code from above will throw an exception.

\subsection{Auxiliary Variables}
When we model a problem as CNF formula, we often need a lot of
additional variables, which do not directly model parts of the problem
(as vertex is selected or not), but that model structural things of
the formula (to allow us to write them in short CNF). These variables
arise a lot and will be added by different methods to the
formula. However, if we talk about the formula on a higher level, we
actually do not want these variables. For instance, we do not want
have variables in our model that we do not know.

\Jdrasil\ provides the concept of \emph{auxiliary variables} to mark
variables as helper variables, that are not directly connected to the
modeled problem. The variable $x_3$ can be marked as auxiliary with the
following command:
\begin{lstlisting}[language=Java]
  phi.markAuxiliary(3);
\end{lstlisting}
Once a variable $x$ is marked as being auxiliary, the following will
happen:
\begin{enumerate}
  \item The variable list of the formula will not contain $x$.
  \item A model will not contain an entry for $x$.
  \item The auxiliary variable list will contain $x$.
  \item The behavior of the formula, a registered SAT solver, and the
    satisfiability of the formula will \emph{not} change. The variable
    is still part of the formula.
\end{enumerate}

\subsection{Cardinality Constraint}
Many problems can naturally be encoded into an CNF~–~\emph{when} we can
restrict the number of variables that we are allowed to set to
true. For instance, a \emph{vertex cover} of a graph is a subset of
its vertices, such that every edge is incident to one of these
vertices. The graph at the border, for instance, has the vertex cover $\{2,4\}$.
\marginpar{
  \tikz[]\graph[spring electrical layout, node distance=0.75cm]{1--2--{3,4},4--{5,6}};
}
For a given graph $G=(V,E)$, it is easy to write down a formula that
states that the graph has a vertex cover:
\[
   \phi=\bigwedge_{\{u,v\}\in E}(x_u\vee x_v).
\]
However, as simple this formula is, as uninteresting it is as well:
every graph contains a vertex cover~–~just take all the vertices. To make
the problem interesting (and difficult), we have to restrict the
number of vertices that we are allowed to set to true. This is exactly
what a \emph{cardinality constraint} does.

\Jdrasil\ provides two ways to add cardinality constraints to a
formula. In both cases, we first of all need so specify the set of
variables (or literals) that we wish to restrict:
\begin{lstlisting}[language=Java]
  // build the formula
  Formula phi = new Formula();
  phi.addClause(1, -2, -3);
  phi.addClause(2, -3);
  phi.addClause(3);
  
  // define a set that we wish to restrict
  Set<Integer> vars = new HashSet<>();
  vars.add(1);
  vars.add(2);
  vars.add(3);
\end{lstlisting}
Note that $\phi$ is satisfiable and has only one model, which sets all
variables to true. So we get:

\codeWithOutput{phi.isSatisfiable()}{true}

We can now restrict the number of variables that are allowed to be set
to true, for instance, to $2$:
\begin{lstlisting}[language=Java]
  phi.addAtMost(2, vars);
\end{lstlisting}
We now obtain, as expected:

\codeWithOutput{phi.isSatisfiable()}{false}

In a similar manner, we can also enforce that a certain amount of
variables \emph{must be set}, but for our formula this has no effect:
\begin{lstlisting}[language=Java]
  phi.addAtLeast(2, vars);
\end{lstlisting}
Both methods, \detail{These methods use sequential counters and
  introduced \textcolor{jdrasil.fg}{$O(kn)$} auxiliary variables \emph{per call}.}
\JMethod{addAtMost} and \JMethod{addAtLeast}, add
clauses and auxiliary variables to the formula. This should be used
for cardinality constraints that are used only once, since these
methods will add these clauses for every call again (even if the set
of variables does not change).

However, when we solve an optimization problem, we often wish to add a
cardinality constraint for the same set of variables again and
again. For instances, a typical routine to solve vertex cover would
look like:
\begin{lstlisting}[language=Java]
  Formula phi = ...       // as in the example
  phi.registerSolver();

  Set<Integer> vars = ... // all variables
  int k = vars.size() - 1;

  phi.addAtMost(k, vars);
  while (phi.isSatisfiable()) {
    k = k - 1;
    phi.addAtMost(k, vars);
  }

  System.out.println(k+1);
\end{lstlisting}
In such an incremental setup, the methods from above are not optimal,
since they would add the similar auxiliary clauses and variables over
and over again. To overcome this, \Jdrasil\ also provides
\emph{incremental cardinality constraints}. The following ensures that
at least $3$ and at most $6$ variables of the set \JMethod{vars} is
set to true:
\begin{lstlisting}[language=Java]
  phi.addCardinalityConstraint(3,6,vars);
\end{lstlisting}
This \detail{This method uses sorting networks and introduces
  \textcolor{jdrasil.fg}{$O(n\log^2 n)$} auxiliary variables overall.}
method will add a lot of auxiliary variables and clauses (most of
the time more than a single call for of \JMethod{addAtMost}), however,
it will reuse this. More precisely, while the first call adds a lot of
structure to the formula, incremental calls will only add single
clauses. So for the algorithm from above, this method is way more
efficient. 

Finally, \Jdrasil\ provides a third possibility to use cardinality
constraints. While computing tree decompositions, we often deal with
instances of small tree width (as the usual use case is
parameterized complexity). If $k$ is much smaller than $n$, a sorting
network is asymptotically not optimal in sense of introduced auxiliary
variables. For such scenarios, the \JClass{Formula} class provides the
method \JMethod{addDecreasingAtMost}.\detail{This method uses a
  variant sequential counter as well. It introduces \textcolor{jdrasil.fg}{$O(kn)$} auxiliary
  variables overall.}This method can be seen as a compromise between
\JMethod{addAtMost} (which is simple, but static), and
\JMethod{addCardinalityConstraint} (which is complex, but can be
decreased and increased between solver calls). In contrast, the method
\JMethod{addDecreasingAtMost} is as simple as the first method, but
allows to be decreased between calls to the solver. It is, however,
only efficient for small values of $k$; and only works for decreasing
upper bounds (and not increasing lower bounds). Note that we can
interstate this as a parameterized SAT encoding, where $k$ is the
parameter.

\section{SAT4J}
The Java Library SAT4J\footnote{\url{http://www.sat4j.org}} is the
most advanced and complete \Lang{SAT}-library for the Java
platform. Although it is not the fastest solver available, it is one
of the most widespread solver, as it has a clean API and a good
documentation.

\Jdrasil\ implements core functionality of SAT4J completely over
reflections. This is done in the \emph{intern} class
\JClass{jdrasil.sat.SAT4JSolver}, which is a
\JClass{jdrasil.sat.ISATSolver}. If the SAT4J library is found in
\Jdrasil's classpath, this class will be used by
\JClass{jdrasil.sat.Formula} (see~\ref{section:satFormula}) to find a
model. This is fully capsuled from the user, which only has to work
with the formula class. 

If SAT4J is available in the classpath,
\JMethod(canRegisterSATSolver()) of \JClass{jdrasil.sat.Formula} will
return true, if SAT4J is also used (this depends on \Jdrasil\ and other
loaded upgrades), the method \JMethod{init()} will return the String
``SAT4J''. 

\subsection{Installation}
To use SAT4J in \Jdrasil, it is sufficient to download the core
library\footnote{\url{http://forge.objectweb.org/project/showfiles.php?group_id=228}}.
We can perform this step automatically with:
\begin{lstlisting}[language=bash]
  ./gradlew upgrade_sat4j
\end{lstlisting}
\subsection{Usage}
Just add the SAT4J core library to the classpath:
\begin{lstlisting}
  java -cp bin:build/upgrades/org.sat4j.core.jar jdrasil.Exact
\end{lstlisting}
Or use one of \Jdrasil's start scripts, which automatically looks for
upgrades in these locations:
\begin{lstlisting}
  ./tw-exact
\end{lstlisting}
Of course, the SAT4J library can be stored at another location as
well. Other than that, just work with the class \JClass{jdrasil.sat.Formula}
as we would otherwise.

\section{Native IPASIR Solver}

Today's most advanced SAT solvers are mostly implemented in
C/C++. \Jdrasil\ can use such ``native'' solver with the help of Javas
JNI-API\footnote{\url{https://docs.oracle.com/javase/7/docs/technotes/guides/jni/spec/jniTOC.html}}. 
To be as general as possible and, thus, to support as many SAT solvers
as possible, \Jdrasil\ implements the
IPASIR\footnote{\url{http://baldur.iti.kit.edu/sat-race-2015/downloads/ipasir.h}}
interface, which is the reversed acronym for ``Re-entrant Incremental
Satisfiability Application Program Interface''. This interface was
proposed and used in recent incremental SAT challenges. 

A solver that implements the IPASIR interface just has to implement
the following 9 functions:
\begin{lstlisting}[language=c]
  const char* ipasir_signature();
  void* ipasir_init();
  void ipasir_release(void* solver);
  void ipasir_add(void* solver, int lit_or_zero);
  void ipasir_assume(void* solver, int lit);
  int ipasir_solve(void* solver);
  int ipasir_val(void* solver, int lit);
  int ipasir_failed(void* solver, int lit);
  void ipasir_set_terminate(void* solver, 
                            void* state, 
                            int (*terminate)(void* state));
\end{lstlisting}
More details about what these functions should do can be found on the
website of recent SAT challenges. The interface is closely related to
the API of modern solvers as Lingeling or PicoSAT, so that such
solvers can easily be linked against IPASIR.

\Jdrasil\ can use an IPASIR solver as upgrade using the class
\JClass{jdrasil.sat.NativeSATSolver}, which implements the interface
\JClass{jdrasil.sat.ISATSolver} with native methods. Note how this
interface, which we also use for other solvers like SAT4J, is closely
related to IPASIR. 

The corresponding C interface of \JClass{jdrasil.sat.NativeSATSolver} can be found
in the header file \file{jdrasil_sat_NativeSATSolver.h}, which is
located in the corresponding subproject folder at \file{subprojects/upgrades/ipasir/}.\note{We can always create this file with
\lstinline[basicstyle=\footnotesize\codefamily]{./gradlew cinterface}} The
corresponding C/C++ implementation
\file{jdrasil_sat_NativeSATSolver.cpp} implements these methods and
maps them against \file{ipasir.h}. This implementation takes care of
keeping \Jdrasil\ and the actual solver in sync, allowing \Jdrasil\ to
use multiple ``instances'' of the solver, and allows \Jdrasil\ to kill
the solver. 

\subsection{Installation} 
To compile an IPASIR upgrade for \Jdrasil, we have to compile the JNI
implementation in the file \file{jdrasil_sat_NativeSATSolver.cpp}
into a dynamic library, which either should be be called
\file{libjdrasil_sat_NativeSATSolver.so} or, depending on you
operating system,
\file{libjdrasil_sat_NativeSATSolver.dylib}. In order to do so, we
have to link against an exiting implementation of an IPASIR solver.

In \file{subprojects/upgrades/ipasir} there is a \file{Makefile} that
compiles the C++-file under the assumption that there is a library
\file{libipasirsolver.dylib} in the same folder. This library should
implement the ipasir interface with an actual SAT solver.

\emph{Using Gradle:} To install a custom IPASIR solver, place a IPASIR compatible
implementation called \file{libipasirsolver.dylib} in
\file{subprojects/upgrades/ipasir} and run
\begin{lstlisting}[language=bash]
  ./gradlew upgrade_ipasir
\end{lstlisting}
in the root folder of \Jdrasil.

\Jdrasil\ is also shipped with two default IPASIR compatible SAT solvers:
glucose\footnote{\url{www.labri.fr/perso/lsimon/glucose/}} and lingeling\footnote{\url{www.fmv.jku.at/lingeling/}}. To upgrade \Jdrasil\ with these state of the
art solvers, use the following commands:
\begin{lstlisting}[language=bash]
  ./gradlew upgrade_glucose
  ./gradlew upgrade_lingeling
\end{lstlisting}
\subsection{Usage}
Once we have compiled \Jdrasil's IPASIR-JNI interface against an
IPASIR solver, i.\,e., once we have a library called
\file{libjdrasil_sat_NativeSATSolver.dylib}, we can \emph{upgrade}
\Jdrasil\ ``on the fly''. \Jdrasil\ will look for the SAT solver in
its library path (not to be confused with its class path), so the
following call will allow \Jdrasil\ to use the solver:
\begin{lstlisting}[language=bash]
  java -cp bin -Djava.library.path=build/upgrades jdrasil.Exact
\end{lstlisting}
Of course, the path can be set to any location, wherever the upgrade
is stored. Note that this only sets the path to location at which
\Jdrasil\ searches the upgrade. If, however, the upgrade is compiled
against other dynamic libraries, these libraries are searched in the
default system depending way (and not in the above specified path).

Alternatively, we can use one of \Jdrasil's premade runscripts:
\begin{lstlisting}[language=bash]
  ./tw-exact
\end{lstlisting}


\bibliography{manual}
\end{document}
